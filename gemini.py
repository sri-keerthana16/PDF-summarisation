import google.generativeai as genai
import os
import textwrap
# from IPython.display import Markdown
genai.configure(api_key="AIzaSyBmYlYbWdiY9yjgqJP8z8Ce2Jd28xxs6MI")
vision_model = genai.GenerativeModel('gemini-pro')


text="""received july 17 2021 accepted august 13 2021 date publication september 3 2021 date current version september 14 2021 digital object identifier 101 109access202131 10143 enhancements attention-based bidirectional lstm hybrid automatic text summarization jiawen jiang 1 haiyang zhang 2 chenxu dai1 qingjuan zhao3 hao feng1 zhanlin ji 14 ivan ganchev 564 1department computer science college arti cial intelligence north china university science technology tangshan 063009 china 2department computer science university shef eld shef eld s10 2tn uk 3department computer science engineering beihang university beijing 100191 china 4telecommunications research centre university limerick limerick v94 t9px ireland 5department computer systems plovdiv university paisii hilendarski '' 4000 plovdiv bulgaria 6institute mathematics informatics bulgarian academy sciences 1040 bulgaria corresponding authors : zhanlin ji ivan ganchev work supported part national key research development program china grant 2017yfe0135700 part grant bg05m2op001-1001-0003 nanced science education smart growth operational program bulgaria co- nanced european union european structural investment funds abstract automatic generation text summary task generating short summary relatively long text document capturing key information past supervised statistical machine learning widely used automatic text summarization task due high dependence quality text features generated summaries lack accuracy coherence computational power involved performance achieved could easily meet current needs paper proposes four novel ats models sequence-to-sequence structure utilizing attention-based bidirectional long short-term memory added enhancements increasing correlation generated text summary source text solving problem out-of-vocabulary words suppressing repeated words preventing spread cumulative errors generated text summaries experiments conducted two public datasets con rmed proposed ats models achieve indeed better performance baselines state-of-the-art models considered index terms natural language processing automatic text summarization sequence- to-sequence model attention mechanism bidirectional lstm pointer network coverage mechanism mixed learning objective function introduction advance mobile internet recent years text information showing trend explosive growth people continuously exposed massive text information time become imperatively important able extract valuable content text information quickly accurately order get good reading experience lower time needed digesting obtained information automatic text summarization aiming automatically outputting concise uent text summaries utilizing different algorithms provided ef cient feasible solution ats aims nd key points understanding text contents order generate smooth associate editor coordinating review manuscript approving publication arianna d'ulizia exible short texts means cutting splicing synonymous reporting abbreviation extension etc ats divided three main types : extrac- tive ats extract combine important words source text order generate text summary ; abstractive ats utilizes neural network algorithms generate concise aggregated text summary means synonymous substitution person conversion imi- tation expansion etc ; hybrid ats employing elements extractive abstractive ats compared extractive ats 
abstractive ats produce text summaries closer produced people 's thinking process utilizing advantages extractive abstractive ats however hybrid ats potential achieve even better results generally challenging task computer understand text contents generate 123660this work licensed creative commons attribution 40 license information see 9 2021j jiang et al : enhancements attention-based bi-lstm hybrid ats text summarization automatically ability understand content information whole text also identify importance text words make choices according importance copy rearrange combine choices meet requirements automatic text generation development deep learning especially emergence recurrent neural networks elab- orated abstractive ats models mainly rely large amount data rather using complex model structures achieve better rapid natural language processing multi- ple elds machine translation speech recognition sequence generation etc however due problem long-distance dependence '' rnn model could lose information certain number steps resulting inaccurate text summarization combining attention mechanism rnn emphasis could put important information selectively ignoring contents generated text summaries would targeted accurate rnn word entered model accord- ing order appearance original text good recording sequence information text limits speed network training text summary generation encoder decoder adopt con- volutional neural network allow model use parallel computing phase train- ing maintain accuracy also improve ef ciency time multi-step attention used every layer semantic vector generated decoder would look forward could improve accuracy supervised statistical machine learning widely used task text summary generation due high dependence quality text features generated summaries often lack accuracy coherence addition computational cost performance easily meet needs massive texts big data ' era therefore taking attention-based bidirectional long short-term memory model foundation aimed improving employing enhanced semantic network pointer network coverage mechanism mixed learning objective function improve automatic generation text summaries reduce word recurrence eg generated traditional statistical methods general models proposed paper except esn belong hybrid ats addition using core principles abstractive ats also employ elements extractive ats processing words unregistered basic vocabulary case choice made copying theunregistered word source text pointing another similar word basic vocabulary based prob- ability produced softmax normalization words proposed hybrid ats models select output terms byusing probability based parts \x15 vocabulary source text rest paper organized follows next section presents related work done eld section iii describes four novel ats models proposed section iv presents performance evaluation elaborated ats models comparison baselines state-of-the-art models discusses results obtained conducted experiments finally section v draws conclusions sets future direction research ii related work 2014 sequence-to-sequence model pro- posed google 's brain team cially started research end-to-end network nlp eld model encoder bidirectional lstm structure whereas decoder unidirectional lstm structure however long-distance dependence '' problem resulted insuf cient accuracy generated text summarization paper published bahdanau et al attention mechanism applied nlp task improving accuracy generated text summaries attention mechanism semantic coding siat time iis direct coding input sequence weighted sum
 considering importance element xjin sequence per following formulav sidxtx jd1bijf ; f denotes coding element xj bijis treated probability ecting importance element xjtosi represented following softmax functionv bijdexp ptx kd1exp ; ejkis degree matching element xjto encoded elements xkin sequence higher degree matching greater uence element greater value bij seq2seq model possible generate unknown words incomplete content solve prob- lems seq2seq integrates keyword information use attention mechanism main information keywords text source considered time improve amount information generated sum- mary another way use hidden state source participating summary vector calculate attention weight summary perception discovering relationship original sen- tences yang et al proposed hierarchical abstraction mech- anism whereby self-attention used rst discover relationship sentences followed repli- cation mechanism solve vocabulary problem volume 9 2021 123661j jiang et al : enhancements attention-based bi-lstm hybrid ats nlp learning representation pioneering research especially applied seq2seq gener- ated output completely depends learning represent source sequence order get rich semantic representation encoder equipped asynchronous bidirectional parallel structure decoder different classi- cal attention-based works uses self-awareness context selection mechanism generate text summaries ef cient way 2015 hu et al applied rnn rnn context model 24 million short-text data contained lcsts cor- pus provided basis generating chinese text summarization dealing noise unimportant information et al proposed srb model whereby source text represented gated attention encoder digest representation generated decoder moreover similarity representations maxi- mized model training process problem unregistered words gu et al proposed copynet model seq2seq type skilfully aided decoder copy mechanism copynet encoder bidirec- tional rnn decoder divided two parts using : generate mode determining output based seman- tics ; copy mode determining copy according position input text decoding process model updates state particular moment uses prediction information previous moment also uses coding information predic- tion output encoding stage cnn used encoder realize word serialization retained accuracy also sped computation model encoder employs cnn bi-lstm decoder bi-lstm structure softmax function resultant cnn-bi- lstm ' model depicted figure 1 figure 1 cnn-bi-lstm ' model due different lengths sentences nlp size input matrix cnn uncertain depends number characters input convolution layer essentially feature extraction layer hyperparameter fis set specify many feature extractors usedfor lter one imagine k\x03dmoving window input matrix kis window size speci ed lter dis length embedding word certain moment nonlinear transformation neural network input value window converted certain characteristic value window continues move backward characteristic value corresponding lter generated continuously form lter feature vector short process extracting features convolution layer lter operates way form different feature extractor pooling layer performs dimensionality reduction operations features lter form nal features generally fully connected layer used pooling layer complete classi - cation process june 2017 google team proposed complete end-to-end processing could realized without using cnn rnn using encoderdecoder attention self-attention manner model encoder decoder employ multi-headed self-attention mecha- nism resultant model called seq2seq catten- tion ' shown figure 2 encoderdecoder attention could used establish corresponding relationship original contents target phrases sentences whereas self-attention pays attention structure word pairs sentence encoder maps original text vector sequence xd context vector decoder uses hidden state hias input query time step decoding queries hidden state encoder time step calcu- lates weight corresponding query performs weighted average obtain context vector u used represent original text information relevant output current time step entering next time step corresponding decoded words inputted cyclic neural network decoder together context vector current concerned information extracted instead relying entirely hidden state previous time step figure 2 seq2seq cattention ' model theory seq2seq cattention ' model gener- alized processing longer sentences currently still limited eld translation 123662 volume 9 2021j jiang et al : enhancements attention-based bi-lstm hybrid ats pointer network variant seq2seq model network perform sequence conversion generates series pointers elements input sequence ats pointer networks used mainly solve oov problem word sparseness issue another successfully used model attention-based bidirectional lstm model presented called bi-lstmcattention ' model encoder employs attention mechanism bi-lstm decoder bi-lstm structure model consists layers : input layer used input data model embedding layer used map word low-latitude space bi-lstm layer uses bidirectional lstm advanced feature extraction weight vector generation attention layer able generate sentence-level fea- tures combining weight vector lexical features output layer basically bi-lstm cattention ' model adds atten- tion mechanism top bi-lstm layer pur- poses applying attention weighting added attention mechanism differentiate weight word thus enable whole sequence get key information easily bi-lstmcattention ' seq2seq cattention' models used basis paper elaborating four new ats models described next section utilizing enhanced semantic network pointer network coverage mechanism mixed learning objec- tive function respectively improve automatic generation text summaries figure 3 bi-lstm cattention ' model main contributions paper could summarized follows : 1 novel esn model proposed using hidden-layer state encoder decoder work semantic similarity loss function paying attention source text model training improvingthe correlation generated text summary source text 2 novel da-pn ' model proposed based joint application attention distribution ends encoder decoder along integrating attention distribution current time step introducing pointer network decoder solving problem unregistered words 3 novel da-pn ccover ' model proposed based coverage mechanism integrating coverage vector calculated attention distribution encoder decoder previous time step along adding loss function avoiding repetition words generated text summaries 4 novel da-pnccovercmlo ' model proposed setting mixed learning objectives along introducing self-critical gradient algorithm setting global reward taking evaluation index part model iteration preventing spread cumulative errors generated text summaries iii proposed models automatic text summarization aims generating concise clear text summary automatically orig- inal text content inputted utilized algo- rithm input text sequence containing mwords andxdcolumns output summarization sequence containing nwords yd y1 ; y2 ; \x01\x01\x01 ; yj ; \x01\x01\x01 ; yn\x03 columns length lnof text summary shorter length lmof source text ie lnlm words xiandyjall come xed vocabulary vof sizejvj word source text could described one-hot vector xi2f0 ; 1gvfor i2f1 ; \x01\x01\x01 ; mg ithword vector length vis used represent whereby ithelement 1 elements 0 order strengthen relationship source text generated text summary section propose rst use enhanced semantic network secondly handle out-of-vocabulary words pro- pose use pointer network integrating decoder attention connect context vector decoder context vector generated source text jointly regulating selection source text additional lexicon thirdly repeated insertions propose add multi-attention coverage mechanism simultaneously using coverage vectors encoder decoder affect attention degree moreover attention information decoder inputted mapping layer input model could pay attention past information thus reduce recurrence phenomenon finally aiming readability generated text summaries propose utilize mixed learning objective function setting global reward speci c discrete gradient 
 could max- imized scoring thus enhancing readability generated text summaries volume 9 2021 123663j jiang et al : enhancements attention-based bi-lstm hybrid ats esn model based hypothesis existence strong correlation generated text summary source text novel esn model proposed works semantic similarity encoder decoder maximizes semantic relevance model training performance enhancement figure 4 proposed esn model esn model last hidden-layer state encoder considered semantic vector geof source text moreover encoder adopts bidirectional lstm one get ged\x10 ehn ; hn\x11 namely features source sequence obtained forward backward direction time nal hidden-layer state obtained unidirectional lstm decoder treated semantic vector gdof generated text summary semantic similarity relesncan expressed calculating cosine similarity geandgd source text generated text summary located semantic space cosine similarity effectively measure distance semantic sim- ilarity could calculated taking account feature vectors decoder encoder directions along calculated cosine similarity averaging value prevent excessive noisev relesnd1 20 behn\x03gd ehn kgdkc hn\x03gd hn kgdk1 ca : new loss function formed original loss function loss td\x00logpw\x03 calculated semantic similarity per asv lossd1 nxn td1loss t\x00\x15rel esn ; pw\x03 denotes probability distribution nal prediction vocabulary \x15denotes regulating factor valued according experience encoder carries information forward backward direction able represent feature source text comprehensively way introducing semanticsimilarity loss function probability generating accurate text summary could increased model training thus improving correlation generated text summary source text b da-pn ' model aiming solving problem unregistered words novel da-pn ' model proposed subsection utilizing decoder attention based pointer network time step model decides whether copy word source text point another similar word existing basic vocabulary former action controlled probability normalized softmax whereas latter relates predicting words input data attention distribution decoder combined unregistered words also predicted appear source text included basic vocabulary figure 5 proposed da-pn ' model shown figure 5 da-pn ' model two input layers respectively copying word basic vocab- ulary source text dimension represents one word switching mechanism p_gen used generating probability controlling input source decoder attention compensate information weakening caused long sequences thus enabling model locate key information accurately attention distribu- tion corresponding probability distribution word source text thus revealing words important prediction process could calculated : tdsoftmax\x10 v tan h\x10 w st t\x11\x11 ; w v denote weight parameters stdenotes hidden-layer state decoder time step weighted sum attention distributions obtained decoder state current time step 123664 volume 9 2021j jiang et al : enhancements attention-based bi-lstm hybrid ats constitutes nal decoder context vector per : ud tdxt\x001 jd1 jsj : probability switch pgencan worked based encoderdecoder context vector current hidden- layer state : pgend\x1bwyyj\x001cweue tcwdud tcbtptr ; wydenotes weight matrix previous word yj\x001wedenotes weight matrix encoder hidden state current time step ue tdenotes nal encoder context vector ud tdenotes nal decoder context vector wddenotes weight matrix decoder hidden state current time step btptrdenotes bias term weight matrix bias term parameters updated model training iteration linear weighted sum parameters nonlinearly activated sigmoid function mapped 0 1 soft switch control source input layer based information two parts \x15 source text vocabulary vocabulary distribution current time step expressed per followsv pvocabdsoftmax ; stis current state sequence vis weight matrix btis bias term training iteration wis predicted word probability nal prediction word wisv p nalwdpgenpvocabwcx ivwidwai ; pgenis probability controlling input source pvocab distribution words nal output vocabulary andp ivwidwairepresents encoder context vector basically refers sum two products : product probability generating new word distribution words vocabulary ; product probability occurrence word source text probability copying way vocabulary expanded form union words original corpus initial vocabulary used express probability distribution expanded vocabulary way limitation using preset basic vocabulary seq2seq model easily overcome proposed da-pn ' model also much easier nd suitable words copying source text relevant weight needs assigned word moreover words selected expanded vocabulary created demand copying words unregistered basic vocabulary source text instance low-frequency words names people places may present basic vocabulary could extracted source text inclusion text summaryin addition da-pn ' : smaller vocabulary could used results saving computation power storage ; model training faster ; fewer epochs1are needed achieve identical performance seq2seqcattention ' consequently simultaneous atten- tion paid encoder decoder generated text summary accurately matches corre- sponding source text c da-pn ccover ' model subsection proposes combine da-pn ' model presented previous subsection coverage mecha- nism integrating multi-attention resultant model named da-pnccover ' depicted figure 6 using attention distribution encoder decoder previous time steps two coverage vectors worked : cd representing attention target sequence ie attention words generated process decoding ; ce representing attention words source text degree coverage certain word sum attentions obtained particular moment initial stage coverage vectors set 0 coverage occurring time figure 6 proposed da-pn ccover ' model adding attention mechanism decoder effec- tively lead focusing features generated text summary however problem word recurrence one pay attention whole sequence controlled global vector addition hidden layer decoder uses context vector past time calculation current input information obtained past used calculation thus strengthening past current time relationship current time step order use source text information past target text summary 's words 1epoch - hyperparameter de ning number times model worked entire training set volume 9 2021 123665j jiang et al : enhancements attention-based bi-lstm hybrid ats generated two vectors calculated previous step integrated attention mechanism per following formulav et jdvttanh\x10 whhjcwsstcwece tcwdcd t\x11 cbtatten ; vtis weight matrix iterative nonlinear acti- vation function used training whis weight matrix iterative hidden state training hjis hidden state ws weight matrix iterative current state sequence training stis current state sequence werepresents weight matrix encoder hidden state current time step wdrepresents weight matrix decoder hidden state current time step ce tis coverage vector encoder cd tis coverage vector decoder btatten bias term training iteration way output current time step affected previous source text generated text summary's words avoid paying attention information thus avoiding recurrence meanwhile context vectors two kinds attention integrated probability distribution vocabulary makes easier model locate important words calculating probability distribution words finally one needs introduce additional loss term punish coverage vector cijand new attention allocation aij get coverage loss function formula task text summary generation one require original texts covered needs punish recurrence attention distribution coverage distribution ie select smaller value two vectors cover __loss tis bounded nal loss function consists original loss coverage loss followsv loss td\x00log\x00 p\x00 w\x03 t\x01\x01 c cover __loss ; pw\x03 denotes probability distribution nal predicted vocabulary cover __loss tdenotes coverage loss function denotes weight coverage mecha- nism overall loss weight coverage mechanism overall loss regulated controlling value moreover introducing distribution attention ends well using additional loss items model effectively suppress possible repeated fragments improve automatic generation text summaries da-pn ccover cmlo ' model order prevent spread cumulative errors gen- erated text summaries subsection proposes add mixed learning objective da-pn c cover ' model resultant model named da-pn c covercmlo ' evaluation indicator rouge recall- oriented understudy gisting evaluation used part model iteration along global rewardhowever rouge differentiable used directly gradient calculation order eliminate limitations self-critical policy gradient training algorithm could adopted generate two independent output sequences training iteration greedy search performed time step probability distri- bution ysofp\x00 ys ijys 1 ; \x01\x01\x01ys i\x001 ; x\x01 decoder 's time step baseline output ycan obtained maximizing output probability distribution de ning ryas reward function output sequence yand comparing real sequence y\x03 one get following formulav lrld\x00 r\x00 y\x01 \x00r\x00 ys\x01\x01xn id1log\x00 p\x00 ys ijys 1 ; \x01\x01\x01 ; ys i\x001 ; x\x01\x01 xdenotes input vector ydenotes baseline output obtained maximizing output probability dis- tribution r denotes reward function output sequence minimized lrlis equivalent conditional likelihood maximized sampling sequence ys gets higher return baseline y\x03 one expect return model improvement global reward used depicted figure 7 figure 7 training process proposed da-pn ccover cmlo' model iv experiments datasets two datasets used experiments conducted proposed ats models : lcsts short-text cor- pus created chinese micro-blogging website sina weibo ; ttnews long-text corpus cre- ated nlpcc 20172018 shared summarization tasks lcsts corpus consists 24 million text\x15 summary ' pairs chinese short texts along correspond- ing short summaries provided authors divided three parts : 2400591 text\x15summary ' pairs used training set ; 10666 human-labeled text\x15summary' pairs scores ranging 1 5 indicating rele- vance short text corresponding summary ; part used validation set ; 1106 text\x15 summary ' pairs scores ranging 1 5 used test set experiments validation set test set use pairs scores less 3 123666 volume 9 2021j jiang et al : enhancements attention-based bi-lstm hybrid ats ttnews corpus largest corpus sin- gle document summarization chinese contains : 50000 news articles browsed toutiao app corresponding summaries written experts 50000 news articles without summary \x15 training set ; 2000 news articles \x15 test set addition randomly extracted 2000 news articles training set used validation set purposes providing unbiased evaluation models ' training set hyperparameters ' tuning early stop- ping training error validation set increases sign tting ttnews articles came different elds sport food entertainment politics technology nance etc b evaluation metrics adopted rouge \x15 set metrics soft- ware package used evaluating automatic summarization nlp based recall accuracy popular rouge evaluation metrics include : rouge-n evaluat- ing overlap n-grams eg rouge-1 unigrams rouge-2 bigrams etc rouge-l longest common subsequence-based statistics rouge-s advan- tages rouge-n come intuitiveness conciseness ection word order n 3 rouge-n value usually small advantage rouge-l require continuous matching words requires matching order appear- ance however calculates longest subsequence nal value ignores uence candidate subsequences longer shorter advantage rouge-s considers word pairs arranged order ects sentence-level word order deeply n-gram model however set maximum number jumping words many meaningless word pairs appear conducted experiments performance evaluation comparison different models used calculated rouge-1 rouge-2 rouge-l scores greater score better performance corre- sponding model however rouge used evaluating automatic summarizations english texts mainly adopted special method applying chinese texts according method chinese text summarizations rst converted dictionary following correspond- ing id dictionary summarizations translated sequences composed ids c compared models incorporated enhancements structure bi-lstmcattention ' seq2seq cattention ' models four proposed ats models primarily compared two baselines addition performance comparison made also following state-of-the-art models:1 rnn ' rnn-context ' \x15 two seq2seq models using gated recurrent units encoder decoder main difference two models attention mecha- nism rnn ' 2 srbcattention ' model gated atten- tion encoder achieves high semantic similarity source text generated summary optimizing cosine similarity loss 3 kesg adds keyword network based seq2seqcadversarial learning 4 dual employs dual methods create summary-aware attention weight considering source text generated summary 5 filtering window model directly uses maximum value projection vector alignment position simplify calculation 6 ham based hierarchical attention network replication mechanism added experimental settings proposed models global parameters remaining unchanged experi- ments mainly composed network-layer parameters feature-representation parameters former lstm hidden layer adopts 256 units word vector uses 256 dimensions pointer network decoder handle characteristics unregistered words rst 50000 words selected basic vocabulary source according word frequency comparison con- sistency vocabulary ensured ends encoder decoder lstm network layer weight matrix distribution computing unit generated uniformly distributed random initializer variables iterated starting 0 standard deviation generated 00001 gaussian distribution random initializer bias term also iterated 0 moreover lstm dropout probability set 04 adam algo- rithm adopted gradient optimization batch_ size determining number initial hidden-state vectors lstm beam_ size number possible results current state set 64 4 respectively extra loss set 08 e results discussions rst group experiments conducted proposed ats models based short-text corpus lcsts whereas second group experiments involved also long-text corpus ttnews obtained results shown tables 1 2 respectively bolded proposed ats models results baselines state-of- the-art models taken corresponding literature sources results involving ttnews cor- pus reported literature state-of-the-art models considered omitted table 2 volume 9 2021 123667j jiang et al : enhancements attention-based bi-lstm hybrid ats table 1 performance comparison models table 2 performance comparison models table 3 provides two running cases proposed ats models showing best performing one among da-pnccovercmlo' results shown tables 1 2 following observations could made : 1 four proposed ats models outperform base- lines bi-lstmcattention ' seq2seq catten- tion ' addition baselines need training time seen figure 8 order reach convergence loss 26 bi-lstm cattention ' seq2seqcattention ' need trained 40 17 epochs respectively whereas proposed ats models need less training ie esn needs 9 epochs da-pn ' \x15 5 epochs da-pn ccover ' da-pnccovercmlo ' \x15 8 epochs 2 two baselines seq2seq cattention ' shows better performance proving general effective use decoder complex network statistical method 3 among proposed ats models rstly esn obvi- ously superior seq2seq cattention ' emphasizes strong correlation head- word source text coherence cor- related according semantic phase speed makes easier get headword pay atten- tion low-frequency words certain extentfurthermore adding semantic model bias could restrained performance improved due increasing correlation source text generated text summary 4 results second proposed model da-pn' con rmed use decoder 's pointer net- work indeed improve model performance effec- tively even smaller basic vocabulary initially used expanded help dealing unregistered words moreover encoder effectively utilize attention information decoder 5 multi-attention coverage mechanism added third proposed model da-pn ccover ' per- fectly solve problem word recurrence even better performance da-pn ' could achieved 6 utilizing mixed learning objective func- tion last proposed model da-pn c covercmlo ' showed ats performance could improved even 7 even though proposed ats models per- form well state-of-the-art models room improving performance espe- cially da-pnccovercmlo ' eg 123668 volume 9 2021j jiang et al : enhancements attention-based bi-lstm hybrid ats table 3 sample summaries generated proposed models figure 8 comparison proposed models baselines terms convergence optimization evaluation indexes order solve problem insuf cient attention decoder subject future research limitations de ciencies proposed ats models overcome future follows : 1 although semantic similarity used mining features could utilized eg part speech location semantic understanding etc 2unk esn model represents unregistered words2 ends used proposed ats models utilize small number network layers whereas deepening neural network would allow models learn information 3 relatively simple reward function used hybrid learning rouge index opti- mization reward function explored research direction future v conclusion paper put forward enhancements struc- ture attention-based bi-directional lstm model attention-based sequence model order improve auto- matic text summarization firstly novel enhanced semantic network model proposed works semantic similarity encoder decoder maximizes semantic relevance train- ing increases probability generating accu- rate text summaries thus also improving correlation source text secondly aiming solving problem unregistered words novel da-pn ' model proposed utilizes decoder attention based pointer network addition simultaneous attention paid encoder decoder resulting volume 9 2021 123669j jiang et al : enhancements attention-based bi-lstm hybrid ats accurate text summaries thirdly proposed combine elaborated da-pn ' model cover- age mechanism integrating multi-attention resultant da-pnccover ' model using attention distribution encoder decoder previous time steps attention current time step affected positive way leading nding accurate words inclusion text summaries avoiding repeated words lastly order prevent spread cumulative errors gen- erated text summaries proposed add mixed learning objective function da-pn c cover ' model resultant da-pn ccovercmlo' model best performing one among ats models proposed paper considers evaluation part model iteration along global reward thus increasing readability generated text summaries performance four proposed ats models compared two baselines bi-lstm cattention' seq2seqcattention ' seven state-of-the-art mod- els using short-text long-text corpora obtained experimental results demonstrated superiority elab- orated ats models compared baselines state-of-the-art models using blended learning mlo best performing proposed model da-pn c covercmlo ' opens prospective improve accuracy automatically generated text summaries opti- mization evaluation indexes could effectively solve problem insuf cient attention decoder research direction explored future"""




def summary(model,text):
   prompt="""You are a summary assistant.Give concise summary of text that includes.
   1.Title of the paper
   2.Authors of the paper
   3.concise summary of the text
   4.Limitations discussed in the text"""
   response = model.generate_content([prompt,text])
   return response
r=summary(vision_model,text)

# r=str(r.parts[0])
def gemini_format(text):
    # Replace '**' with empty space
    text = text.replace('**', '')
    
    # Split the text into lines
    lines = text.split('\n')
    
    # Initialize Markdown formatted text
    markdown_text = ''
    
    # Iterate over the lines and format them as Markdown
    for line in lines:
        # If the line starts with '*', it's a bullet point text
        if line.startswith('*'):
            markdown_text += f"- {line[2:]}\n"
        # If the line starts with '**', it's a title
        elif line.startswith('**'):
            markdown_text += f"{line[2:]}:\n\n"
        # Otherwise, keep the line as is
        else:
            markdown_text += f"{line}\n"
    
    return markdown_text
# print(type(r))

# r=r.split('\n')
# r='\n'.join(r)

# r=r.replace("**","").replace("text:","")
print(gemini_format(str(r.parts[0])))

# print(r)

